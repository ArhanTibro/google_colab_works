{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOPcOEqFCQG0ysYrQmyWUU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyyWw1iOkjAI"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel,DistilBertConfig\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and Preprocess IMDB Dataset\n",
        "# Load a smaller subset for speed\n",
        "train_data, test_data = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split=['train[:1000]', 'test[-200:]'],\n",
        "    as_supervised=True\n",
        ")\n",
        "\n",
        "# Convert to Python lists\n",
        "train_texts, train_labels = [], []\n",
        "for text, label in train_data:\n",
        "    train_texts.append(text.numpy().decode('utf-8'))\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "test_texts, test_labels = [], []\n",
        "for text, label in test_data:\n",
        "    test_texts.append(text.numpy().decode('utf-8'))\n",
        "    test_labels.append(label.numpy())\n"
      ],
      "metadata": {
        "id": "rlNo_jwrl2d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Text\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_texts(train_texts)\n",
        "test_encodings = tokenize_texts(test_texts)\n"
      ],
      "metadata": {
        "id": "4wAoM1bP6POA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': train_encodings['input_ids'],\n",
        "        'attention_mask': train_encodings['attention_mask']\n",
        "    },\n",
        "    train_labels\n",
        ")).batch(16)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': test_encodings['input_ids'],\n",
        "        'attention_mask': test_encodings['attention_mask']\n",
        "    },\n",
        "    test_labels\n",
        ")).batch(16)\n"
      ],
      "metadata": {
        "id": "_G43oBKu6bHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Build the Transfer Learning Model\n",
        "\n",
        "\n",
        "# Custom layer that wraps DistilBERT\n",
        "class DistilBertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DistilBertLayer, self).__init__(**kwargs)\n",
        "        self.bert = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "\n",
        "# Define input layers\n",
        "input_ids = tf.keras.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
        "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "\n",
        "# BERT encoder\n",
        "cls_output = DistilBertLayer()(inputs)\n",
        "\n",
        "# Classification head\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(cls_output)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "output = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "# Define the final model\n",
        "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9abcjKyAEwn",
        "outputId": "8da03843-d6d6-49c1-d9bc-92b9e170ec33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the Model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "-SCPiUZ8AK5J",
        "outputId": "3fa6dca1-ee71-411b-a76b-d53412b73745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ distil_bert_layer_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDistilBertLayer\u001b[0m)   │                   │            │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m49,216\u001b[0m │ distil_bert_laye… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m130\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ distil_bert_layer_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DistilBertLayer</span>)   │                   │            │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,216</span> │ distil_bert_laye… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m49,346\u001b[0m (192.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,346</span> (192.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m49,346\u001b[0m (192.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,346</span> (192.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=3\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB6u0xJpATD4",
        "outputId": "af2a9ad1-3214-4636-af01-3079a4822197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 4s/step - loss: 0.7275 - sparse_categorical_accuracy: 0.5240 - val_loss: 0.6753 - val_sparse_categorical_accuracy: 0.5550\n",
            "Epoch 2/3\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 4s/step - loss: 0.6826 - sparse_categorical_accuracy: 0.5704 - val_loss: 0.6530 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 3/3\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 4s/step - loss: 0.6621 - sparse_categorical_accuracy: 0.5774 - val_loss: 0.6374 - val_sparse_categorical_accuracy: 0.6400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model\n",
        "results = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {results[0]:.4f}, Test Accuracy: {results[1]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCTLiOKxKvEU",
        "outputId": "addc7f0a-d3b6-401e-fb55-f2c85133ea45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3s/step - loss: 0.6437 - sparse_categorical_accuracy: 0.6318\n",
            "Test Loss: 0.6374, Test Accuracy: 0.6400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict Sentiment\n",
        "\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to analyze\n",
        "        model: Your trained Keras model\n",
        "        tokenizer: The DistilBERT tokenizer\n",
        "\n",
        "    Returns:\n",
        "        tuple: (predicted sentiment, confidence score)\n",
        "    \"\"\"\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # Make prediction\n",
        "    outputs = model.predict({'input_ids': inputs['input_ids'],\n",
        "                           'attention_mask': inputs['attention_mask']})\n",
        "\n",
        "    # Get probabilities and predicted class\n",
        "    probs = outputs[0]\n",
        "    predicted_class = np.argmax(probs)\n",
        "    confidence = np.max(probs)\n",
        "\n",
        "    # Convert to human-readable labels\n",
        "    sentiment = \"positive\" if predicted_class == 1 else \"negative\"\n",
        "\n",
        "    return sentiment, float(confidence)\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\"\n",
        "sentiment, confidence = predict_sentiment(sample_text, model, tokenizer)\n",
        "print(f\"Sentiment: {sentiment} (Confidence: {confidence:.2f})\")\n",
        "\n",
        "sample_text2 = \"I hated this film. It was boring and the characters were poorly developed.\"\n",
        "sentiment2, confidence2 = predict_sentiment(sample_text2, model, tokenizer)\n",
        "print(f\"Sentiment: {sentiment2} (Confidence: {confidence2:.2f})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8q-EBtfLRLI",
        "outputId": "9542346e-04d5-4a74-c0c9-85088ee77b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
            "Sentiment: positive (Confidence: 0.57)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n",
            "Sentiment: negative (Confidence: 0.68)\n"
          ]
        }
      ]
    }
  ]
}